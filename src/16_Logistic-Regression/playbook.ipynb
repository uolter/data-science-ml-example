{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We have an anonymized data set of about 200 users, containing each user’s salary, her years of experience as a data scientist, and whether she paid for a premium account.\n",
    "\n",
    "As is usual with categorical variables, we represent the dependent variable as either 0 (no premium account) or 1 (premium account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0),(8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0),(1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),(6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),(8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),(0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),(0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),(8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0),(1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),(7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),(8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),(4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),(8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),(2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),(8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),(2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),(2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),(7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),(8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),(5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),(4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),(4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),(3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),(8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),(1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),(9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),(7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),(2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),(5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]\n",
    "data = list(map(list, data)) # change tuples to lists\n",
    "\n",
    "x = [[1] + row[:2] for row in data] # each element is [1, experience, salary]\n",
    "y = [row[2] for row in data]        # each element is paid_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious first attempt is to use linear regression and find the best model:\n",
    "\n",
    "paid account = β 0 + β 1 experience + β 2 salary + ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGLNJREFUeJzt3X+U3XWd3/HnK5kMJkCAJCQCLnGbKBSPSHIUo9AyLgSi\nW1cLnhW1NtCqnBa27pF2wVIP6Vk4ytmze1alridKZetqXSvdFXUJqMto/cEKFQRrArgoKlhYXRCX\nAob47h/3O/EmmR93vpmZOzc8H+fcM98fn+/n+/nc78x9zffnTVUhSdJ0Leh3AyRJg8kAkSS1YoBI\nkloxQCRJrRggkqRWDBBJUit9D5Ak1yR5KMmdE8x/Y5JvNa+vJHnhXLdRkrSvvgcI8BHgrEnm3wf8\n06p6EXAF8KE5aZUkaVJD/W5AVX0lyepJ5t/SNXoLcMzst0qSNJX5sAcyHW8Bbuh3IyRJ82APpFdJ\nXgGcD5za77ZIkgYkQJKcCGwFNlXVI5OU88FekjRNVZU2y82XQ1hpXvvOSI4FrgPeXFV/O1VFVXVA\nvi6//PK+t8H+2T/7d+C99kff90CSfBwYAZYn+QFwOTAMVFVtBd4FLAM+kCTAzqo6uV/tlSR19D1A\nquqNU8x/K/DWOWqOJKlH8+UQlqYwMjLS7ybMKvs32OzfM1P29xjYfJKkDqT+SNJsS0IN+El0SdKA\nMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogk\nqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqZW+B0iSa5I8lOTOScq8L8m9\nSe5IctJctk+SNL6+BwjwEeCsiWYmeSWwpqqeB1wAfHCuGqb+uvHGGznzzHM488xzuPHGGyecPlG5\nXupfv36E5cvXsn79qXvUtX79COvXn7pHnVdeeSVLlhxJsoIlS47myiuv3D19+fK1LF++lvPOO4+j\njz6OZAULFx7O+vXrWbJkZbPMUZx33nmsXbuOoaHO/OXLn8tBBy0nOZxkMQsWdMqNtWfMeeedx6JF\nq1i0aBUbN26csL9jfVq69FiWLDmSpUtXs3btiaxd+wKWLj2WRYtWkhxEsowFC5azdu0LdtfR9n0c\ndM/Ufs+Iqur7C1gN3DnBvA8Cr+8a3w6smqBs6cCwbdu2Wrx4VcG1BdfW4sWratu2bftMHx4+vIaH\nj9ynXC/1dy8HK2po6OB9psHFtXjxqtq8eXPBkmba2PyldcYZZxQs3WMaXNwMLys4aJ9lfjV/RTO8\nrPnZPe+IGh4+vLZt29ase+91nLJPf8fr06/qHVv+4n3qGho6uK644opx3+8D3US/Z88kzedmu8/u\ntgvO5GuKAPkM8PKu8S8A6ycoOzPvqPpu48azmz/qal7X1saNZ48zfcO45drUD88ZZ1qn3NDQynHX\nBcsnWGaqOvesf8+fY/M21MaNZzfr3ruOlfv0d/w+nb1Xu8fv97Jla1q9j4Nuot+zZ5L9CZChudvX\nmRtbtmzZPTwyMsLIyEjf2iJJ883o6Cijo6MzU1nb5JnJF9M7hLUDD2Ed8DyE5SGsueAhrP3bA+l7\neHTaz3OBuyaY9yrgc83wBuCWSeqZmXdU88K2bdt2H7bq/qPee/pE5Xqpf92602rZsjW1bt0pe9S1\nbt1ptW7dKXvU2fmQXVGwvBYvPqquuOKK3dOXLVtTy5atqc2bN9dRRz2/YHktWHBYrVu3rhYvPrJZ\n5tm1efPmWrPmpFq4sDN/2bLVNTy8rOCwgmdV0ik31p4xmzdvrqGhlTU0tLLOOOOMCfs71qdDD/21\nWrx4RR166LG1Zs0La82aE+rQQ3+thoaOLBguOKKSZbVmzQl7BFCb93HQPVP7PWZ/AiSd5fsnyceB\nEWA58BBwOTBMp1NbmzJXA5uAx4Hzq+qbE9RV/e6PJA2SJFRVWi17IH3gGiCSND37EyDz4T4QSdIA\nMkAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogk\nqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWql7wGSZFOS\nHUnuSXLJOPOXJrk+yR1J7kpyXh+aKUnaS6qqfytPFgD3AKcDDwK3AudW1Y6uMu8EllbVO5OsAO4G\nVlXV0+PUV/3sjyQNmiRUVdos2+89kJOBe6vq/qraCXwCeM1eZQo4tBk+FPjpeOEhSZpb/Q6QY4Af\ndo3/qJnW7WrghCQPAt8C3j5HbZMkTWKo3w3owVnA7VX1G0nWAJ9PcmJV/cN4hbds2bJ7eGRkhJGR\nkTlppCQNgtHRUUZHR2ekrn6fA9kAbKmqTc34pUBV1VVdZT4LvLuqvtqMfxG4pKpuG6c+z4FI0jQM\n8jmQW4G1SVYnGQbOBa7fq8z9wBkASVYBzwfum9NWSpL20ddDWFW1K8lFwE10wuyaqtqe5ILO7NoK\nXAFcm+TOZrHfq6q/71OTJUmNvh7CmmkewpKk6RnkQ1iSpAFlgEiSWjFAJEmtGCCSpFYMEElSKwaI\nJKkVA0SS1IoBIklqxQCRJLVigEiSWjFAJEmtGCCSpFYMEElSKwaIJKkVA0SS1IoBIklqxQCRJLVi\ngEiSWjFAJEmtGCCSpFYMEElSKwaIJKkVA0SS1ErfAyTJpiQ7ktyT5JIJyowkuT3Jt5PcPNdtlCTt\nK1XVv5UnC4B7gNOBB4FbgXOrakdXmcOArwFnVtUDSVZU1U8mqK/62R9JGjRJqKq0WbbfeyAnA/dW\n1f1VtRP4BPCavcq8Ebiuqh4AmCg8JElzq98Bcgzww67xHzXTuj0fWJbk5iS3JnnznLVOkjShoX43\noAdDwHrgN4CDga8n+XpVfXe8wlu2bNk9PDIywsjIyBw0UZIGw+joKKOjozNSV7/PgWwAtlTVpmb8\nUqCq6qquMpcAz6qq/9yMfxi4oaquG6c+z4FI0jQM8jmQW4G1SVYnGQbOBa7fq8yngVOTLEyyBHgp\nsH2O2ylJ2ktfD2FV1a4kFwE30Qmza6pqe5ILOrNra1XtSHIjcCewC9haVd/pY7MlSUxxCCvJz4Hx\nCoTOB/zS2WpYGx7CkqTp2Z9DWJPugVTVoe2aJEk60E3rEFaSlcCzxsar6gcz3iJJ0kDo6SR6kt9K\nci/wPeBLwPeBG2axXZKkea7Xq7B+H9gA3FNVv07n0SO3zFqrJEnzXq8BsrOqfgosSLKgqm4GXjyL\n7ZIkzXO9ngN5NMkhwJeBjyV5GHh89polSZrveroTPcnBwJN0Lt99E3AY8LFmr2Te8DJeSZqe/bmM\nt6+PMplpBogkTc+s3QfStYLuGwqHgUXA4/PtRkJJ0tzpKUC6byhMEjrf2bFhtholSZr/Wh/CSnJ7\nVa2b4fbsFw9hSdL0zMUhrLO7RhfQuYT3yTYrlCQdGHq9jPfVXcNP07kTfe+vnpUkPYP0GiAfrqqv\ndk9Icgrw8Mw3SZI0CHq9E/39PU6TJD1DTLoHkuRlwMuBI5O8o2vWUmDhbDZMkjS/TXUIaxg4pCnX\n/d0gjwGvm61GSZLmv14fZbK6qu6fg/bsFy/jlaTp2Z/LeHs9B/LhJId3rfCI5nvKJUnPUL0GyIqq\nenRspKoeAVbOTpMkSYOg1wD5ZZJjx0aSPJdfPRtLkvQM1Ot9IJcBX0nyJTqPdP8nwNtmrVWSpHmv\n52dhJVlJJzRuBxYDD1fVl2exbdPmSXRJmp5ZP4me5C3AF4GLgX8PfBTY0maF49S9KcmOJPckuWSS\nci9JsnOv53JJkvqk13MgbwdeAtxfVa8A1gGPTr7I1JIsAK4GzgJeALwhyfETlHsP4JVfkjRP9Bog\nT1bVkwBJDqqqHcBxM7D+k4F7q+r+qtoJfILxH9L4O8Cn8NlbkjRv9HoS/UfNfSB/CXw+ySPATNxY\neAzww+710AmV3ZIcDby2ql6RZI95kqT+6fUbCf95M7glyc3AYcC2WWvVnv4Y6D43MunJni1btuwe\nHhkZYWRkZFYaJUmDaHR0lNHR0Rmpq/U3Es7IypMNwJaq2tSMXwpUVV3VVea+sUFgBfA48Laqun6c\n+rwKS5KmYX+uwup3gCwE7gZOB34MfAN4Q1Vtn6D8R4DPVNX/nGC+ASJJ0zDrX2k7W6pqV5KLgJvo\nnNC/pqq2J7mgM7u27r3InDdSkjSuvu6BzDT3QCRpeubiabySJO3BAJEktWKASJJaMUAkSa0YIJKk\nVgwQSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEi\nSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqZW+B0iSTUl2JLknySXjzH9jkm81r68keWE/\n2ilJ2lOqqn8rTxYA9wCnAw8CtwLnVtWOrjIbgO1V9bMkm4AtVbVhgvqqn/2RpEGThKpKm2X7vQdy\nMnBvVd1fVTuBTwCv6S5QVbdU1c+a0VuAY+a4jZKkcfQ7QI4Bftg1/iMmD4i3ADfMaoskST0Z6ncD\nepXkFcD5wKmTlduyZcvu4ZGREUZGRma1XZI0SEZHRxkdHZ2Ruvp9DmQDnXMam5rxS4Gqqqv2Knci\ncB2wqar+dpL6PAciSdMwyOdAbgXWJlmdZBg4F7i+u0CSY+mEx5snCw9J0tzq6yGsqtqV5CLgJjph\ndk1VbU9yQWd2bQXeBSwDPpAkwM6qOrl/rZYkQZ8PYc00D2FJ0vQM8iEsSdKAMkAkSa0YIJKkVgwQ\nSVIrBogkqRUDRJLUigEiSWrFAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWrF\nAJEktWKASJJaMUAkSa0YIJKkVgwQSVIrBogkqRUDRJLUigEiSWql7wGSZFOSHUnuSXLJBGXel+Te\nJHckOWmu2yhJ2tdQP1eeZAFwNXA68CBwa5JPV9WOrjKvBNZU1fOSvBT4ILChLw2eBTfeeCN/+Idb\nAbj44rdx1llnzUg9wLj1Tra+sXk/+clPeeyxv+ORR55i9erncM45G/nSl74JwNFHH8p1193EE088\nzfDwQo4/fg3vfve7ALjwwku5777vU7UTGAaeZsECGBpaxEEHHczKlYcDu3jooUd44omn2LWrgJ3A\nE8DiZpmdwK7dy8NCFix4kl/+8hCGhxewfPlifvrTJ/jFL54AngIOAX7ZlB0GfgYcBvyCzq93mnkL\nmnJDwOPAwU2vn2zWvQvYxeLFSzn++Gdz110P8PTTTwOPNvUV8ATJISTFwoW/YGjoUHbteopdu8Lw\n8CIOP/xZPPXUQo444lCWLj2Yxx57jIcffpRFi4Z5xzvO57LLLtvj/T/ttPW739eLL34bt912G3/0\nRx8B4NWvPpUHH/z5uNtJmjeqqm8vOkFwQ9f4pcAle5X5IPD6rvHtwKoJ6qtBsm3btlq8eFXBtQXX\n1uLFq2rbtm37Xc/w8JE1PHz4PvVOtr6958HSgoub19Ku6UsKlnWNH1FDQwfXwoVHdE1b0Sy3oll2\nrK5DmuW7y45NWzHB8mPrP6erXecUHLRXu7rLntJDfeP1bbL5e5cda8eK2vc9Gxtetse8zZs3d73H\ne9Y3NHTYhOtq+3sh9aL53Gz3Gd52wZl4AecAW7vG/wXwvr3KfAZ4edf4F4D1E9Q3U+/pnNi48ezm\nQ6Ka17W1cePZM1IPbNin3snWN34dZzev7ukbxin3nAmWHWvHhmZ8wzjLjzete/mxn2u65q3p+uAe\nb5mVPdQ3Xt8mmz9e2TUTLD/++zQ0tHKS+jZMuq42vxdSL/YnQPp6CGs2bNmyZffwyMgIIyMjfWuL\nJM03o6OjjI6OzkxlbZNnJl50DmFt6xrv5RDWDjyENWk9HsLyEJbUK/ZjDySd5fsjyULgbjon0X8M\nfAN4Q1Vt7yrzKuDCqvrNJBuAP66qcU+iJ6l+9qcNT6J7En1sW3gSXf2QhKpKq2X7/YGbZBPwXjp/\n4ddU1XuSXEAnFbc2Za4GNtH5yz+/qr45QV0DFyCS1E8DHSAzyQCRpOnZnwDp+42EkqTBZIBIklox\nQCRJrRggkqRWDBBJUisGiCSpFQNEktSKASJJasUAkSS1YoBIkloxQCRJrRggkqRWDBBJUisGiCSp\nFQNEktSKASJJasUAkSS1YoBIkloxQCRJrRggkqRWDBBJUisGiCSplb4FSJIjktyU5O4kNyY5bJwy\nz0ny10n+T5K7kvy7frRVkrSvfu6BXAp8oaqOA/4aeOc4ZZ4G3lFVLwBeBlyY5Pg5bOO8MTo62u8m\nzCr7N9js3zNTPwPkNcCfNsN/Crx27wJV9X+r6o5m+B+A7cAxc9bCeeRA/wW2f4PN/j0z9TNAVlbV\nQ9AJCmDlZIWTPBc4CfibWW+ZJGlKQ7NZeZLPA6u6JwEF/Kdxitck9RwCfAp4e7MnIknqs1RN+Lk9\nuytOtgMjVfVQkmcDN1fVPx6n3BDwWeCGqnrvFHX2pzOSNMCqKm2Wm9U9kClcD5wHXAVsBj49Qbn/\nCnxnqvCA9m+CJGn6+rkHsgz4JPBrwP3Ab1fVo0mOAj5UVf8sySnAl4G76BziKuA/VtW2vjRakrRb\n3wJEkjTYBvZO9CSvS/LtJLuSrJ+k3KYkO5Lck+SSuWzj/ujlRsum3PeTfCvJ7Um+MdftnK5etkeS\n9yW5N8kdSU6a6zbuj6n6l+S0JI8m+WbzGu+CknkpyTVJHkpy5yRlBnnbTdq/Ad92Pd2UPe3tV1UD\n+QKOA55H5ybE9ROUWQB8F1gNLALuAI7vd9t77N9VwO81w5cA75mg3H3AEf1ub499mnJ7AK8EPtcM\nvxS4pd/tnuH+nQZc3++2tuzfqXQupb9zgvkDu+167N8gb7tnAyc1w4cAd8/E397A7oFU1d1VdS+d\nS4MncjJwb1XdX1U7gU/QuYFxEEx5o2UjDM6eZC/b4zXAfwOoqr8BDkuyisHQ6+/bQF7sUVVfAR6Z\npMggb7te+geDu+16uSl72ttvUD542joG+GHX+I8YnDvZe73RsoDPJ7k1yVvnrHXt9LI99i7zwDhl\n5qtef99e1hwi+FySE+amaXNikLddrwZ+201yU/a0t18/L+Od0iQ3Il5WVZ/pT6tmzgzdaHlKVf04\nyZF0gmR785+U5qf/DRxbVf8vySuBvwSe3+c2qTcDv+1m+qbseR0gVbVxP6t4ADi2a/w5zbR5YbL+\nNSfzVtWvbrR8eII6ftz8/Lskf0HnMMp8DZBetscDdC7tnqzMfDVl/7r/aKvqhiQfSLKsqv5+jto4\nmwZ5201p0Lddc1P2p4CPVtV4991Ne/sdKIewJjoueSuwNsnqJMPAuXRuYBwEYzdawgQ3WiZZ0vxH\nQZKDgTOBb89VA1voZXtcD/xLgCQbgEfHDuUNgCn7131MOcnJdC6lH4gPoEaY+O9tkLfdmAn7dwBs\nu6luyp729pvXeyCTSfJa4P3ACuCzSe6oqld234hYVbuSXATcRCcsr6mq7X1s9nRcBXwyyb+iudES\noLt/dA5//UXzCJch4GNVdVO/GjyVibZHkgs6s2trVf1Vklcl+S7wOHB+P9s8Hb30D3hdkn8D7ASe\nAF7fvxZPT5KPAyPA8iQ/AC4HhjkAth1M3T8Ge9udArwJuCvJ7TQ3ZdO5YrD19vNGQklSKwfKISxJ\n0hwzQCRJrRggkqRWDBBJUisGiCSpFQNEktSKASLNkiQ/b34eleSTU5R9e5JnTbP+05IM/CN9NLgM\nEGkakkznb6ag87iZqvrtKcr+LrCkRZO8kUt9Y4BIjeYRJNuT/FmS7yT5ZJLFSb6X5D1JbqNzN/I/\nSnJD8wTkLyV5frP8c5N8LZ0v+Pr9veq9qxlekOQPmi/1uSPJhUl+BzgauDnJF5tyZzZ13Zbkz5Ms\naaZvatp4G3D2XL9HUjcDRNrTccDVVXUC8Bjwb+n8l/+TqnpxVX0S2ApcVFUvAf4D8CfNsu8F/ktV\nvQj48V71ju0pXEDn8REnVtVJdB4/8346D60bqarTkywHLgNOr6oX03kK7DuSHNSs+zeb6c+ejTdA\n6tXAPgtLmiU/qKpbmuGPAWNf/fnnsPuhlS8H/keSsYfuLWp+nsKv9go+CrxnnPpPB/6kmmcIVdWj\nzfTuh/htAE4AvtqsYxHwdeB44L6quq8p92fAfP8OGB3ADBBpcmN7Do83PxcAj1TV+gnKjpXfn2+u\nC3BTVb1pj4nJi/azXmlGeQhL2tOxSV7aDL8R+F/dM6vq58D3krxubFqSE5vBrwJvaIb3+PDv8nng\ngiQLm2WPaKY/Bixthm8BTkmypimzJMnzgB3A6iS/3pR7A1IfGSDSnu4GLkzyHeAw4IPjlHkT8K+b\nk+DfBn6rmf67zbLfAo6aoP4P0/na0Dubx2qPhcCHgG1JvlhVP6HzKO3/3tT1NeC4qnqKzjmUv2pO\nog/ad23oAOPj3KVGktXAZ6vqhf1uizQI3AOR9uR/VFKP3AORJLXiHogkqRUDRJLUigEiSWrFAJEk\ntWKASJJaMUAkSa38fwBRnm0Ub5HvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f84a4d048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from working_with_data import rescale\n",
    "from multiple_regression import estimate_beta, predict\n",
    "import math, random\n",
    "\n",
    "rescaled_x = rescale(x)\n",
    "beta = estimate_beta(rescaled_x, y) # [0.26, 0.43, -0.43]\n",
    "predictions = [predict(x_i, beta) for x_i in rescaled_x]\n",
    "plt.scatter(predictions, y)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We’d like for our predicted outputs to be 0 or 1, to indicate class membership. It’s fine if they’re between 0 and 1, since we can interpret these as probabilities an output of 0.25 could mean 25% chance of being a paid member. But the outputs of the linear model can be huge positive numbers or even negative numbers, which it’s not clear how to interpret. Indeed, here a lot of our predictions were negative.\n",
    "\n",
    "The linear regression model assumed that the errors were uncorrelated with the columns of x. But here, the regression coefficent for experience is 0.43, indicating that more experience leads to a greater likelihood of a premium account. This means that our model outputs very large values for people with lots of experience. But we know that the actual values must be at most 1, which means that necessarily very large outputs (and therefore very large values of experience ) correspond to very large negative values of the error term. \n",
    "Because this is the case, our estimate of beta is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic Function\n",
    " \n",
    "In the case of logistic regression, we use the [logistic function](https://en.wikipedia.org/wiki/Logistic_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from machine_learning import train_test_split\n",
    "from collections import Counter\n",
    "from functools import partial, reduce\n",
    "from gradient_descent import maximize_stochastic, maximize_batch\n",
    "from linear_algebra import vector_add, dot\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    return logistic(x) * (1 - logistic(x))\n",
    "\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)))\n",
    "\n",
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "               for x_i, y_i in zip(x, y))\n",
    "\n",
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"here i is the index of the data point,\n",
    "    j the index of the derivative\"\"\"\n",
    "\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"the gradient of the log likelihood\n",
    "    corresponding to the i-th data point\"\"\"\n",
    "\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j)\n",
    "            for j, _ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add,\n",
    "                  [logistic_log_gradient_i(x_i, y_i, beta)\n",
    "                   for x_i, y_i in zip(x,y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta stochastic [-1.906182482651773, 4.053083869373743, -3.8788953691426906]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_x, y, 0.33)\n",
    "\n",
    "# want to maximize log likelihood on the training data\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "gradient_fn = partial(logistic_log_gradient, x_train, y_train)\n",
    "\n",
    "# pick a random starting point\n",
    "beta_0 = [1, 1, 1]\n",
    "\n",
    "# and maximize using gradient descent\n",
    "beta_hat = maximize_batch(fn, gradient_fn, beta_0)\n",
    "print(\"beta stochastic\", beta_hat)\n",
    "\n",
    "true_positives = false_positives = true_negatives = false_negatives = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.9333333333333333\n",
      "recall 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "for x_i, y_i in zip(x_test, y_test):\n",
    "    predict = logistic(dot(beta_hat, x_i))\n",
    "\n",
    "    if y_i == 1 and predict >= 0.5:  # TP: paid and we predict paid\n",
    "        true_positives += 1\n",
    "    elif y_i == 1:                   # FN: paid and we predict unpaid\n",
    "        false_negatives += 1\n",
    "    elif predict >= 0.5:             # FP: unpaid and we predict paid\n",
    "        false_positives += 1\n",
    "    else:                            # TN: unpaid and we predict unpaid\n",
    "        true_negatives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a precision of 93% (“when we predict paid account we’re right 93% of the time”) and a recall of 82% (“when a user has a paid account we predict paid account 82% of the time”), both of which are pretty respectable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
