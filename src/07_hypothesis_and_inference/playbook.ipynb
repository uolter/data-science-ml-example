{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Hypothesis Testing\n",
    "\n",
    "We want to test whether a certain hypothesis is likely to be true.\n",
    "\n",
    "**Hypotheses** are assertions like “this coin is fair” or “data scientists prefer Python to R” or “people are more likely to navigate away from the page without ever reading the content if we pop up an  advertisement that can be translated into statistics about data.\n",
    "\n",
    "Those **statistics** can be thought of **observations** of random variables from known distributions, which allows us to make statements about how likely those assumptions are to hold.\n",
    "\n",
    "We have a **null hypothesis** $H_{0}$ that represents some default position, and some **alternative hypothesis** $H_{1}$ that we’d like to compare it with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Flipping a Coin\n",
    "\n",
    "We have a coin and we want to test whether it’s fair.\n",
    "\n",
    "The coin has come probability p of landing heads, and so null hypothesis is that the coin is fair which corresponds to p = 0.5\n",
    "\n",
    "$H_{0}$: p=0.5\n",
    "\n",
    "$H_{1}:  p \\neq 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.stats\n",
    "\n",
    "def normal_approximation_to_binomial(n, p):\n",
    "    \"\"\"finds mu and sigma corresponding to a Binomial(n, p)\"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma\n",
    "\n",
    "def normal_cdf(x, mu=0,sigma=1):    \n",
    "    return scipy.stats.norm(mu, sigma).cdf(x)\n",
    "\n",
    "\n",
    "# the normal cdf _is_ the probability the variable is below a threshold\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# it's above the threshold if it's not below the threshold\n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# it's between if it's less than hi, but not less than lo\n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# it's outside if it's not between\n",
    "def normal_probability_outside(lo, hi, mu=0, sigma=1):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the reverse—find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood. For example, if we want to find an interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability (leaving 60%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    #return scipy.stats.norm(mu, sigma).pdf(p)\n",
    "    return scipy.stats.norm(mu, sigma).ppf(p)\n",
    "\n",
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the z for which P(Z <= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the z for which P(Z >= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the symmetric (about the mean) bounds that contain the specified probability\"\"\"\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    # upper bound should have tail_probability above it\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    # lower bound should have tail_probability below it\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say that we choose to flip the coin n = 1000 times. If our hypothesis of fairness is true, X should be distributed approximately normally with mean 500 and standard deviation 15.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu= 500.0 sigma 15.811388300841896\n"
     ]
    }
   ],
   "source": [
    "print(\"mu=\", mu_0, \"sigma\",sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How willing we are to make a **type 1 error** (“false positive”), in which we reject H 0 even though it’s true. \n",
    "For reasons lost to the annals of history, this willingness is often set at 5% or 1%. Let’s choose 5%.\n",
    "\n",
    "Consider the test that rejects $H_{0}$ if X falls outside the bounds given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds  469.010248385 530.989751615\n"
     ]
    }
   ],
   "source": [
    "print(\"Bounds \", lo, hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95000000000000018"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_probability_between(lo, hi, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just a 5% chance we observe an X that lies outside this interval, which is the exact significance we wanted.\n",
    "\n",
    "If $H_{0}$ is true, then, approximately 19 times out of 20, this test will give the correct result.\n",
    "\n",
    "We are also often interested in **the power of a test**, which is the probability of not making a type 2 error, in which we fail to reject $H_{0}$ even though it’s false.\n",
    "\n",
    "Let’s check what happens if p is really 0.55, so that the coin is slightly biased toward heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low  469.010248385  hi  530.989751615\n"
     ]
    }
   ],
   "source": [
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "print(\"Low \", lo, \" hi \", hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mu  550.0  sigma  15.732132722552274\n"
     ]
    }
   ],
   "source": [
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\n",
    "\n",
    "print(\"Mu \", mu_1, \" sigma \", sigma_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power  0.886547781098\n"
     ]
    }
   ],
   "source": [
    "# a type 2 error means we fail to reject the null hypothesis. Which will happen when X is still \n",
    "# in our original interval\n",
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "\n",
    "print(\"power \", power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine instead that our null hypothesis was that the coin is not biased toward heads, or that p ≤ 0.5. \n",
    "In that case we want a one-sided test that rejects the null hypothesis when X is much larger than 50 but not when X is smaller than 50.\n",
    "So a 5% significance test involves using normal_probability_below to find the cutoff below which 95% of the probability lies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi  526.007419394\n",
      "Power  0.936378997858\n"
     ]
    }
   ],
   "source": [
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "print(\"hi \", hi)\n",
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "\n",
    "print(\"Power \", power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **more powerful test**, since it no longer rejects $H_{0}$ when X is below 469 (which is very unlikely \n",
    "to happen if $H_{1}$ is true) and instead rejects $H_{0}$ when X is between 526 and 531"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of thinking about the preceding test involves **p-values**. Instead of choosing bounds based on some probability cutoff, we compute the probability—assuming $H_{0}$ is true—that we would see a value at least as extreme as the one we actually observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        # if x is greater than the mean, the tail is what's greater than x\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "\n",
    "    # if x is less than the mean, the tail is what's less than x\n",
    "    return 2 * normal_probability_below(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to see 530 heads, we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.062077215796\n"
     ]
    }
   ],
   "source": [
    "print(two_sided_p_value(529.5, mu_0, sigma_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to convince yourself that this is a sensible estimate is with a **simulation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06005\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "extreme_value_count = 0\n",
    "\n",
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0 for _ in range(1000))\n",
    "    \n",
    "    if num_heads >= 530 or num_heads <= 470:\n",
    "        extreme_value_count += 1\n",
    "    \n",
    "print(extreme_value_count / 100000)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is greater than our 5% significance, we don’t reject the null hypothesys. If we instead saw 532 heads, the p-value would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Confidence interval\n",
    "\n",
    "\n",
    "A third approach is to construct a confidence interval around the observed value of the parameter.\n",
    "For example, we can estimate the probability of the unfair coin by looking at the average value of the Bernoulli variables corresponding to each flip—1 if heads, 0 if tails. If we observe 525 heads out of 1,000 flips, then we estimate p equals 0.525.\n",
    "\n",
    "How confident can we be about this estimate? Well, if we knew the exact value of p, the central limit theorem tells us that the average of those Bernoulli variables should be approximately normal, with mean p and standard deviation:\n",
    "\n",
    "```\n",
    "math.sqrt(p * (1 - p) / 1000)\n",
    "```\n",
    "Here we don’t know p, so instead we use our estimate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1-p_hat) / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the normal approximation, we conclude that we are **“95% confident”** that the following interval contains the true parameter p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval [0.4940490098153452 0.5559509901846548]\n"
     ]
    }
   ],
   "source": [
    "lo, hi = normal_two_sided_bounds(0.95, mu, sigma)\n",
    "\n",
    "print(\"Confidence interval [{0} {1}]\".format(lo, hi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we’d seen 540 heads, then we’d have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.50910957476724517, 0.57089042523275491)\n"
     ]
    }
   ],
   "source": [
    "p_hat = 540 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print(normal_two_sided_bounds(0.95, mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, “fair coin” doesn’t lie in the confidence interval. (The “fair coin” hypothesis doesn’t pass a test that you’d expect it to pass 95% of the time if it were true.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-hacking\n",
    "\n",
    "A procedure that erroneously rejects the null hypothesis only 5% of the time will—by definition—5% of the time erroneously reject the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    return [random.random() < 0.5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "experiments = [run_experiment() for _ in range(1000)]\n",
    "\n",
    "num_rejections = len([experiment for experiment in experiments if reject_fairness(experiment)])\n",
    "\n",
    "print(num_rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Running an A/B Test\n",
    "\n",
    "Try to get people to click on advertisements. One of your advertisers has developed a new energy drink targeted at data scientists, and the VP of Advertisements wants your help choosing between advertisement A (“tastes great!”) and advertisement B (“less bias!”).\n",
    "\n",
    "You decide to run an experiment by randomly showing site visitorsone of the two advertisements and tracking how many people click on each one.\n",
    "Let’s say that $N_{A}$ people see ad A, and that $n_{A}$ of them click it. We can think of each adview as a **Bernoulli** trial where $p_{A}$ is the probability that someone clicks ad A. Then (if $N_{A}$ is large, which it is here) we know that $n_{A}/N_{A}$ is approximately a normal random variable with mean $p_{A}$ and standard deviation $\\sigma _{A} = \\sqrt{p_{A}(1-p_{A})/N_{A}}$.\n",
    "\n",
    "Similarly, $n_{B}/N_{B}$ is approximately a normal random variable with mean $p_{B}$ and standard deviation $\\sigma _{B} = \\sqrt{p_{B}(1-p_{B})/N_{B}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimated_parameters(N, n):\n",
    "    p = n / N\n",
    "    sigma = math.sqrt(p * (1 - p) / N)\n",
    "    return p, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume those two normals are **independent** (which seems reasonable, since the individual Bernoulli trials ought to be), then their difference should also be normal with mean $p_{B} − p_{A}$ and standard deviation $\\sqrt{p_{A}^{2} + p_{B}^{2}}$\n",
    "\n",
    "This means we can test the **null hypothesis** that $p_{A}$ and $p_{B}$ are the same (that is, that $p_{A} − p_{B}$ is zero), using the statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a_b_test_statistic(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    \n",
    "    return (p_B - p_A) / math.sqrt(sigma_A**2 + sigma_B**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if “tastes great” gets 200 clicks out of 1,000 views and “less bias” gets 180\n",
    "clicks out of 1,000 views, the statistic equals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1403464899034472\n"
     ]
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 180)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of seeing such a large difference if the means were actually equal would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25414197654223603"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is large enough that you can’t conclude there’s much of a difference. On the other hand, if “less bias” only got 150 clicks, we’d have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.948839123097944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0031896997062168583"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 150)\n",
    "print(z)\n",
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means there’s only a 0.003 probability you’d see such a large difference if the ads were equally effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
